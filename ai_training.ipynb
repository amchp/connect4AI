{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers, Input\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from connect4 import Connect4\n",
    "\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 10  # Limit training episodes, will run until solved if smaller than 1\n",
    "\n",
    "connect_4 = Connect4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 7\n",
    "possible_actions = [i for i in range(7)]\n",
    "\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(43,)),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(num_actions, activation=\"linear\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "num_games = 1\n",
    "max_games = 1000000\n",
    "trainging_game_batch_size = 100\n",
    "reward_punishment_const = 100\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_games = 500\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_games = 1000\n",
    "\n",
    "update_network = 1000\n",
    "\n",
    "# History\n",
    "games_states = []\n",
    "games_next_states = []\n",
    "games_actions = []\n",
    "games_rewards = []\n",
    "games_done = []\n",
    "\n",
    "loss_function = keras.losses.Huber()\n",
    "\n",
    "def transform_state(state):\n",
    "  return tf.convert_to_tensor(\n",
    "      np.array(state)\n",
    "  )\n",
    "\n",
    "def flatten_list(lst):\n",
    "  return [\n",
    "      lst[i][j] \n",
    "      for i in range(len(lst))\n",
    "      for j in range(len(lst[i]))\n",
    "  ]\n",
    "\n",
    "while num_games < max_games:\n",
    "  state = connect_4.reset()\n",
    "  state = transform_state(state)\n",
    "\n",
    "  done = False\n",
    "  game_states = []\n",
    "  game_next_states = []\n",
    "  game_actions = []\n",
    "  game_rewards = []\n",
    "  game_done = []\n",
    "  \n",
    "  while not done:\n",
    "    if num_games < epsilon_random_games or epsilon > np.random.rand(1)[0]:\n",
    "      action = np.random.choice(num_actions)\n",
    "    else:\n",
    "      action_probs = model(state, training=False)\n",
    "      action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "\n",
    "    epsilon -= epsilon_interval / epsilon_greedy_games\n",
    "    epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "    next_state, reward, done = connect_4.move(action)\n",
    "    game_states.append(state)\n",
    "    game_next_states.append(next_state)\n",
    "    game_actions.append(action)\n",
    "    game_rewards.append(reward)\n",
    "    game_done.append(abs(done))\n",
    "\n",
    "    next_state = transform_state(next_state)\n",
    "    state = next_state\n",
    "  for i in range(len(game_rewards)):\n",
    "    player = game_states[i][-1]\n",
    "    if player == done:\n",
    "      game_rewards[i] += reward_punishment_const\n",
    "    else:\n",
    "      game_rewards[i] -= reward_punishment_const\n",
    "  games_states.append(game_states)\n",
    "  games_next_states.append(game_next_states)\n",
    "  games_actions.append(game_actions)\n",
    "  games_rewards.append(game_rewards)\n",
    "  games_done.append(game_done)\n",
    "\n",
    "  if len(games_states) == trainging_game_batch_size:\n",
    "    state_sample = np.array(flatten_list(games_states))\n",
    "    next_state_sample = np.array(flatten_list(games_next_states))\n",
    "    action_sample = np.array(flatten_list(games_actions))\n",
    "    rewards_sample = np.array(flatten_list(games_rewards))\n",
    "    done_sample = np.array(flatten_list(games_done))\n",
    "\n",
    "    future_reward = model_target.predict(next_state_sample)\n",
    "    updated_q_values = rewards_sample + gamma * np.amax(\n",
    "      future_reward, axis=1\n",
    "    )\n",
    "    updated_q_values = updated_q_values * (1 - done_sample) - reward_punishment_const*done_sample\n",
    "    \n",
    "    masks = tf.one_hot(action_sample, num_actions)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      q_values = model(state_sample)\n",
    "      \n",
    "      q_action = np.sum(np.multiply(q_values, masks), axis=1)\n",
    "      loss = loss_function(updated_q_values, q_action)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "  if num_games % update_network == 0:\n",
    "    model_target.set_weights(model.get_weights())\n",
    "    print(\"Updated Network\")\n",
    "\n",
    "\n",
    "  num_games += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connect4AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
